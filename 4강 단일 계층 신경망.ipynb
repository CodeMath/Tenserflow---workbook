{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "[인공 뉴런]\n",
    "\n",
    "ex)\n",
    "'사각형'과 '원' 레이블이 달린 점들이 2차원 평면. -> 여기에 점 X가 추가됨\n",
    "-> 쉽게 생각하면 앞서 배운 선형회귀분석으로 분류할 수 있다!\n",
    "(여기서 직선을 따라 위이면 0, 아래면 1로 분류 할 수 있음. 2차원 벡터로 생각)\n",
    "따라서 '선' y = W * x + b (W는 가중치)\n",
    "\n",
    "정리하면, 뉴런은 가중치(입력 데이터 X와 같은 차원을 가짐)와 오프셋 b를 학습 -> 어떻게 점을 분류할까? 배우자!\n",
    "\n",
    "여기서 b는 신경망에서 평향(basis). \n",
    "\n",
    "뉴런은 0 또는 1의 결과를 내기 위해 비선형 활성화 함수(activation function)를 적용\n",
    "\n",
    "z = b + sigma[i] (x_i W_i)\n",
    "y = 1 ( z >= 0) \n",
    "y = 0 ( z < 0 )\n",
    "\n",
    "선형회귀분석과 비슷하게 적용( 뉴런에 레이블 달린 데이터를 공급 -> 결과를 실제 값과 비교 -> 오차 최소화 반복, 가중치 W와 b조정)\n",
    "W와 b 매개변수를 구해서 가중치 합(W * x + b) 계산하고 나면 z 에 저장된 결과를 0 또는 1로 교환 가능\n",
    "\n",
    "* 시그모이드 함수(sigmoid function) - S자 비스듬하게 누은 거처럼 수렴하는 모양의....\n",
    "\n",
    "z = b + sigma[i] (x_i W_i)\n",
    "y = 1/(1+exp(-z))\n",
    "\n",
    "즉, 시그모이드 함수는 0과 1에 가까운 값으로 수렴함.\n",
    "1) z to +inf, exp(-z) to 0 -> y to 1(allmost)\n",
    "2) z to -inf, exp(-z) to +inf -> y to 0(allmost)\n",
    "\n",
    "[단일 계층 신경망(single-layer neural network)]\n",
    "- 입력을 받는 하위계층(입력계층 : input layer)\n",
    "- 결과 값을 내는 상위계층(출력계층 : output layer)\n",
    "등 여러 계층으로 구성\n",
    "\n",
    "여기서 중간계층(은닉계층 : hidden layer)이 존재할 수 도...(알파고가 그랬듯....)\n",
    "=> 이러한 형태를 다중 계층 신경망(multi-layer neural network)\n",
    "\n",
    "\n",
    "[소프트맥스 함수(softmax function) : 출력 계층에서 두 가지 이상의 클래스(여기서는 레이블/카테고리)로 데이터 분류 ]\n",
    "- 시그모이드 함수의 일반화된 형태\n",
    "- 각 클래스에 대한 확률을 얻게 해줌\n",
    "- 클래스의 확률의 합은 1이고, 가장 높은 확률을 가진 클래스가 결과 값이 될 가능성이 높다.\n",
    "\n",
    "- 두 개의 주요 단계로 구성\n",
    "1) 이미지가 어떤 레이블에 속하는지 근거(evidence)를 계산하는 과정\n",
    "2) 근거들을 각 레이블에 대한 확률로 변환하는 것\n",
    "\n",
    "\n",
    "1) 어떻게 근거를 계산할까?\n",
    "- 각 이미지의 픽셀의 진한 정도에 대한 가중치 합을 계산하는 것.\n",
    "- 주어진 클래스(레이블)에 없는 진한 픽셀이 이미지에 있다면, 가중치는 음의 값, 자주 겹치면 양의 값이 되깄지? (yes)\n",
    "\n",
    "2) 위에서 계산한 결과로 확률을 구하자.\n",
    "- 소프트맥스 함수를 이용\n",
    "y = softmax(evidence) \n",
    "\n",
    "softmax(x)_i = exp(x_i) / sigma[j] (exp(x_j))\n",
    "출력 벡터는 원소의 합이 1인 확률 함수가 되어야함. 벡터의 각 원소를 정규화하기 위해 입력 값을 모두 지수 값으로 교체\n",
    "\n",
    "[이해가 안되는 부분 정리]\n",
    "1) 지수함수를 사용하면 가중치를 더 커지게 하는 효과?\n",
    "- 잘 모르겠는뎅...\n",
    "2) 한 클래스의 근거가 작을 때, 이 클래스의 확률도 낮아짐\n",
    "- 그런가? evidence가 작으니 당연한거 같은 느낌인데....?!아닌가? 잘 모르겠당.\n",
    "3) 소프트 맥스는 가중치의 합이 1이 되도록 정규화하여 확률분포를 만듬\n",
    "- 확인해보장\n",
    "\n",
    "* 여러가지 활성화 함수들이 있으니 찾아보자!\n",
    "\n",
    "\n",
    "\n",
    "모델이 충분히 좋은지 혹은 아닌지 판단하기 위해서는 '비용함수'를 사용하는데, 보통은 반대의 경우를 정의.\n",
    "평균제곱오차 나 유클리드 제곱거리를 이용하지만, 여기서는\n",
    "* 교차 엔트로피 에러(cross entropy error) 사용\n",
    "\n",
    "sigma[i] ( y_i' log(y_i) )\n",
    "\n",
    "y는 예측된 확률분포이고, y'은 레이블이 달린 훈련 데이터로부터 얻은 실제 분포\n",
    "* 핵심은 y와 y' 분포가 같을 때 최솟값을 얻는다!\n",
    "\n",
    "반복적인 최소화 과정이 일어나는데, 이러한 문제를 해결하기 위한 여러 알고리즘이 있음.\n",
    "* 오차를 후방으로 전파하는 역전파(backpropagation) 알고리즘 사용\n",
    "- 가중치 W를 재계산할 때, 출력 값으로부터 얻은 오차를 뒤쪽으로 전파. (다중 계층 신경망에서 중요한 알고리즘)\n",
    "- 텐서플로가 모델을 훈련시키기 위해 적절한 비용함수의 기울기를 찾는 최적화 알고리즘\n",
    "- 데이터 그래프를 실행한다\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "#-*-coding:utf-8 -*-\n",
    "# 데이터 다운로드\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(55000), Dimension(784)])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# 이미지에 대한 인덱스 55000 | 이미지 안의 픽셀 수 784\n",
    "tf.convert_to_tensor(mnist.train.images).get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.zeros([784,10]))\n",
    "b = tf.Variable(tf.zeros([10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(\"float\", [None, 784]) # 여기서 None이라고 쓰여진 부분은 어떤 크기나 가능. 학습과정에서 사용될 이미지의 총 개수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = tf.nn.softmax(tf.matmul(x,W) + b) # 소프트맥스 함수를 이용하여 이미지 벡터 x, 가중치 행렬 W , b 를 더함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_ = tf.placeholder(\"float\",[None,10]) # 교차 엔트로피 함수를 구하기 위해, 실제 레이블을 담고 있는 새로운 플레이스홀더"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_entropy = -tf.reduce_sum(y_*tf.log(y)) # 크로스 엔트로피 비용 함수 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_step = tf.train.GradientDescentOptimizer(0.001).minimize(cross_entropy) # 학습속도가 0.01 , 경사하강법 알고리즘 적용\n",
    "# 크로스 엔트로피를 최소화하는 역전파 알고리즘 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(1000)\n",
    "    sess.run(train_step, feed_dict={x:batch_xs, y_:batch_ys})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9213\n"
     ]
    }
   ],
   "source": [
    "print sess.run(accuracy, feed_dict={x:mnist.test.images, y_:mnist.test.labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
